# Workshop Notes 
**Topics:** RAG, Agentic AI, Sprint-1 Backlog & Setup

---

## Recap from Workshop 1

In Workshop 1 we focused on **framing the problem** and getting our collaboration tools ready:

- Reviewed the **PRFAQ Document**
    - Clarified:
        - Who our users are (students, faculty, reviewers, marketing).
        - What problems EDU-AGENT is solving (faster programme analysis, better evidence, guided campaigns).
            
- Discussed **tools and spaces** we’ll work in
    - GitHub (code + issues + project boards)
    - AWS (accounts vended via SSO, CLI profiles)
    - Recording & notes (session videos + docs/workshops in the repo)
        
- Developed a **working flow on GitHub** and reviewed the boards
    - Team A and Team B each have their own **Kanban board**
    - A unified board gives a **single view** for stakeholders
        
- Created an **issue ticket structure**
    - User stories → features → tasks → acceptance criteria
    - Labels for: `frontend`, `backend`, `data`, `docs`, `ux`, `ci`, etc.
        

### Post-Session (Workshop 1)

- Collected and reviewed **student feedback cards**
    - Mapped your skills and interests (backend, frontend, data, DevOps, research/docs).
    - Used these to **balance teams** and assign Week-1 work.
        
- Confirmed **availability and cadence** for weekly meetings and office hours.
    

---

## Today’s Goals (Workshop 2)

1. Build a basic understanding of **RAG** and **Agentic AI**
2. Review our **Week-1 / Sprint-1 plan** and user stories
3. Confirm and refine these user stories into **issues and assignments** on GitHub
4. Walk through the **AWS CLI + SSO** setup so everyone can access their AWS account from the terminal
    
---

## RAG – Retrieval-Augmented Generation

> **RAG = LLM + external knowledge source.**  
> Instead of asking the model to “remember everything”, we retrieve relevant information from our documents and feed it into the model as context.

### From user to output (high-level pipeline)

1. **Question (user input)**  
    A user (student, faculty, reviewer, marketing) asks a question.  
    Example:
    
    > “What are the entry requirements and key outcomes for the Cloud Computing track?”
    
2. **Retrieve documents / knowledge**  
    The system **searches a knowledge base** for relevant content:
    - Programme catalogues
    - Standards (e.g. BQA, ACM)
    - PRFAQ and requirement docs
    - Internal policies, rubrics, previous reviewer feedback
        
3. **Use an LLM to answer from those documents**  
    The model receives:
    - The user’s question
    - The retrieved snippets (chunks) from the documents  
        Then generates an answer **grounded in those snippets**.
        

### Key ideas & vocabulary

- **Corpus** – the full body of documents we care about (PDFs, HTML pages, docs).
- **Chunking** – splitting large documents into smaller pieces (paragraphs/sections) so we can index and retrieve them efficiently.
- **Embeddings/vector store** – numeric representation of chunks that enables “semantic search” (finding similar meaning, not just exact keywords).
- **Retriever** – the component that, given a question, finds the most relevant chunks.
- **Grounded answer** – an answer that can reference and be traced back to your real documents.
    
### Why do we care?

We need answers that are:

- **Accurate** – tied to actual programme documents, not hallucinations.
- **Fresh** – easy to update if the catalogue or standards change (no retraining needed).
- **Controlled** – we decide which sources are allowed (e.g. only UoB docs + selected comparators).

In our context, this is how our system will answer questions about programmes, requirements and policies **with citations** to real sources.

### Summary

> **RAG is a method for ensuring LLM responses are contextually accurate and up to date, by grounding them in an external knowledge base.** [datacamp.com](https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag)

---

#### Dive Deep (optional): Learn more about RAG

If you’d like to go beyond today’s explanation:

- **Conceptual overviews**
    
    - DataCamp – [_What is Retrieval-Augmented Generation (RAG)?_](https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag) 
    - SingleStore –[ _A guide to RAG_ (nice description of how retrieval + generation work together)](https://www.singlestore.com/blog/a-guide-to-retrieval-augmented-generation-rag)    

When you read these, try to **map their components** (corpus, embeddings, retriever, LLM) back onto our EDU-AGENT system.

---

## Agentic AI – AI Agents

> An **AI agent** is more than a single chat response. It is a system where the model can **decide what to do next**, call tools, and act over **multiple steps** toward a goal.

Rather than “one prompt → one response”, an agent:
- Has a **goal** or task (e.g. “analyse this programme and generate a BQA-ready summary”).

- Can **plan** steps (e.g. “first retrieve catalogue; then check standards; then compare to peers; then write a report”).

- Can **call tools**:
    - RAG retriever
    - Trend data fetcher
    - Reporting or email generator
        
- Maintains some **state** (remembers intermediate results, decisions, errors).

### State-based function / state machine

We looked at a simple **state machine outline** for our agent:

`RECEIVE_QUERY -> PLAN -> CALL_TOOLS (RAG / trend / etc.) -> SUMMARISATION -> RESPOND`

Each arrow is a **transition**; each label is a **state**.

Why state machines are useful:
- They make the agent’s behaviour **explicit**.
- You can **debug** by seeing where in the flow something went wrong.
- Later, you can implement this as a workflow (e.g. AWS Step Functions).
    

AWS [Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.htm) is an example of a state-machine-based orchestration service – you define states and transitions in JSON, and it runs the workflow reliably.

### Agents vs workflows

A useful mental model from recent industry guides:
- **Workflows** – more “fixed path”: defined steps in code (e.g. always A → B → C).
- **Agents** – more “dynamic”: the LLM can choose which tool to call next, whether to loop, when to stop. [LangChain Docs+1](https://docs.langchain.com/oss/python/langgraph/workflows-agents)
    

In EDU-AGENT:

- Our **agent** will dynamically choose between:
    - Consulting the RAG corpus
    - Pulling trend data
    - Building/revising a report
        
- But we still wrap that in a **safe, auditable workflow** (guardrails, allowed tools, timeouts).
    

---

#### Dive Deep (optional): Agents & workflows

If you want more depth:

- **Agent patterns and workflows**
    
    - LangChain docs – [_Workflows and agents_](https://docs.langchain.com/oss/python/langgraph/workflows-agents): good conceptual distinction between fixed workflows and dynamic agents.
    - Anthropic –[ _Building effective agents_](https://www.anthropic.com/research/building-effective-agents) (discusses orchestration patterns and when to use agents vs workflows). 
        
- **Agentic workflow examples**
    
    - OpenAI – [_A practical guide to building agents_](https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf) (PDF) – end-to-end patterns for tool-using LLM systems.
    - Patronus / n8n / IBM guides on [agentic workflows](https://www.patronus.ai/ai-agent-development/agentic-workflow) – examples of multi-tool, multi-step agents in practice.

As you read, think: _“Which pattern looks most like what we want EDU-AGENT to do?”_

---

## User Stories and Backlog Creation

> Pipeline: **User Story** (idea from user’s perspective) → **Technical Feature** → **Actionable items** → **Tangible artefacts** (code, docs, UI).

We’re using **user stories** to bridge between:
- What the **university** needs (sponsors, reviewers, students, marketing).
- The **technical work** you’ll do (backend, frontend, data, DevOps, UX).
    

### User stories – reminder

A common template is:

> **As a** _persona_
> **I want** _capability_  
>  **So that** _value/outcome_

Example for EDU-AGENT:

> As a **programme coordinator**,  
> I want to **upload a draft programme and receive alignment feedback**,  
> so that I can **fix issues before sending it to reviewers**.

Good stories are:
- **User-centred** – written from the persona’s point of view.
- **Valuable** – clearly create value for someone.
- **Just detailed enough** – [not a technical spec](https://www.atlassian.com/agile/project-management/user-stories)
    , but clear enough to estimate and break down.

### From story to backlog

For each story we:

1. Identify **features**
    - e.g. “Student Q&A interface”, “RAG corpus ingestion”, “Trend monitor”, “Agent tool contract”.
        
2. Break features into **issues / tasks**
    - e.g. “Design agent tool inventory”, “Implement /ask endpoint stub”, “Create Q&A journey map”.
        
3. Add **acceptance criteria**
    - Clear conditions of satisfaction: what must be true to call the task [“done”](https://www.easyagile.com/blog/how-to-write-good-user-stories-in-agile-software-development)? 
        

In Workshop 2, we confirmed five anchor user stories for Week-1:

- **US-01:** PoC charter, KPIs, and demo storyline
- **US-02:** Agent tool inventory and JSON I/O contracts
- **US-03:** Initial knowledge corpus and RAG design (sources, metadata, vector store)
- **US-04:** Student Q&A experience slice (journey + early UI/UX)
- **US-05:** Repo structure, GitHub workflows, basic CI/dev experience
    
Each of you now has at least one **primary issue** connected to one of these.

---

#### Dive Deep (optional): Writing stronger user stories

If you want to practice writing your own:

- Atlassian – [_User stories with examples and a template_](https://www.atlassian.com/agile/project-management/user-stories)
- Atlassian – [_Epics, stories, themes and initiatives_](https://www.atlassian.com/agile/project-management/epics-stories-themes) (how stories roll up into bigger work).
- Parallel – [_How to write user stories (2025 guide)_](https://www.parallelhq.com/blog/how-to-write-user-stories) – focuses on the “persona, need, value” triad.

Try writing 1–2 **new stories** for EDU-AGENT (e.g. for faculty, marketing, or reviewers) and thinking about how you’d break them into tasks.

---

## Actionable Items (For Today and Sprint-1)

### 0. Important -AWS CLI LOGIN
> Important security note:
> For this challenge, you should not create or store long-term AWS access keys on your laptop. We will only use AWS SSO with temporary credentials via aws configure sso and aws sso login.
>Temporary credentials automatically expire after a short period, which greatly reduces risk if your machine or config files are ever compromised.

We are using AWS IAM Identity Center (SSO) so that everyone works with
SHORT-LIVED, TEMPORARY credentials instead of long-term access keys.

This is an important security requirement for the challenge.

What this means in practice:

- You SHOULD use:
  - aws configure sso
  - aws sso login --profile <your-profile-name>
- You SHOULD NOT:
  - create IAM user access keys in the console,
  - run "aws configure" and paste in access_key_id / secret_access_key.

The SSO flow issues temporary credentials that:
- are cached locally by the AWS CLI,
- automatically expire after a limited time,
- can be refreshed safely with:

  aws sso login --profile <your-profile-name>

This avoids storing long-lived credentials in ~/.aws/credentials and
follows AWS security best practices.

If you ever see lines like this in ~/.aws/credentials or appearing as entries in your terminal:

  aws_access_key_id = ...
  aws_secret_access_key = ...

please let us know and remove them. For this project you should only be
using SSO-based profiles created via "aws configure sso".


These are the concrete steps we expect by the end of Sprint-1.

### 1. Repository setup and access

-  **Repo is set up and accessible**
    - Link: [https://github.com/bahrain-uob/Agentic-AI](https://github.com/bahrain-uob/Agentic-AI)
        
-  **Repo cloned & personal/feature branch created** (per student)
    - Use a naming convention like: `feature/<team>-<short-description>`
        
-  **Access confirmed and local environment set up**
    - Install required tools (e.g. Python/Node, Git, editor/IDE).
    - Able to run simple commands or tests locally.
        

#### Dive Deep (optional): Git & GitHub practice

If you’re still getting comfortable with Git:
- Practice basic commands: `clone`, `status`, `add`, `commit`, `push`, `pull`, `checkout -b`.
- GitHub’s own _Hello World_ and _Git branching_ tutorials are good starting points.
    

---

### 2. Conceptual understanding of Agentic-AI and RAG

-  We have a shared **high-level understanding** of:
    - What RAG is and why we use it.
    - What an agent is and how tools fit into the loop.
        

You will deepen this understanding by **working on your assigned stories**:
- Those on **US-02/US-03** will refine tool contracts and RAG design.
- Those on **US-04** will design the Q&A experience (and see how RAG appears to users).
    
---

### 3. Actionable Sprint-1 goals

By the end of Sprint-1, our goals are:

-  Agree a **minimum spec** for Sprint-1
    - A documented definition of what we expect to be “in place” (docs + design + scaffolding).
        
-  Have a **functional foundation** for the full project
    - Not a full product yet, but:
        - Architecture documented.
        - Tools & components named and described.
        - Repo, CI and basic endpoints/UI skeleton in place.    

Think of Sprint-1 as the **foundation**: if this is solid, later sprints (full agent and RAG implementation, trend integration, dashboards) will be much easier.

---

### 4. AWS CLI & SSO (IMPORTANT: REFER TO SECTION 0 ABOVE)

We walked through:

- Logging in to the AWS account via **SSO** using your university email.
- Installing and verifying the **AWS CLI** (`aws --version`).
- Running `aws configure sso` to create a named CLI profile and then authenticating it with `aws sso login` flow. !For security reasons refer to the Section 0.
- Testing access with:
```bash
  aws sts get-caller-identity --profile <your-profile-name>
  ```
  This ensures everyone is ready to:
- Deploy small components later (e.g., Lambda functions, Step Functions, vector store).
- Experiment safely in their vended AWS accounts.
> If you ever find yourself pasting an aws_access_key_id and aws_secret_access_key into your laptop for this challenge, stop – that’s exactly what we don’t want. Everything should go through SSO and temporary credentials only.
    

#### Dive Deep (optional): AWS CLI & SSO

If you want more practice or got stuck:
- AWS Docs – [_Configuring IAM Identity Center authentication with the AWS CLI_](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sso.html)
- AWS Docs – [_Tutorial: Using IAM Identity Center to run S3 commands in the AWS CLI_](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-sso-tutorial.html)
- DataCamp – [_How to use the AWS CLI: installation, setup, and commands_](https://www.datacamp.com/tutorial/aws-cli-tutorial) (good general intro).

# Closing

These enriched notes are meant to be a **reference document**: you can come back here when you forget a term (RAG, agent, state machine) or when you want something deeper to read while working on your Sprint-1 issues.